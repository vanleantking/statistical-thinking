[
["index.html", "Statistical Thinking: A Simulation Approach to Modeling Uncertainty", " Statistical Thinking: A Simulation Approach to Modeling Uncertainty This website is intended to serve as an organizational hub for the most current version of the CATALST Project’s Statistical Thinking: A Simulation Approach to Modeling Uncertainty. Here you will be able to access materials such as readings, data sets, and the lab manual. It also includes helpful links and resources for each of the course topics. Licensing Copyright © 2017 Catalysts for Change PUBLISHED BY CATALYST PRESS This work is licensed under a Creative Commons Attribution 4.0 International License. You are free to share, remix, and to make commercial use of the work under the condition that you provide proper attribution. To reference this work, use: Zieffler, A., &amp; Catalysts for Change. (2017). Statistical Thinking: A simulation approach to uncertainty (4th ed.). Minneapolis, MN: Catalyst Press. The work to create the material appearing in the book was made possible by the National Science Foundation (DUE–0814433). "],
["introduction.html", "Introduction", " Introduction Learning statistics is sexy. Hal Varian, Google’s chief economist, believes this. During an interview in McKinsey Quarterly, Varian stated, “I keep saying the sexy job in the next ten years will be statisticians. People think I’m joking, but who would’ve guessed that computer engineers would’ve been the sexy job of the 1990s?” Varian is not the only person to express this sentiment either. Hans Rosling in the 2010 BBC documentary Joy of Stats1 referred to statistics as the “sexiest subject around”. Whether you believe it is the sexiest subject or not, it is incontrovertible that the use of statistics and data are prevalent in today’s information age. Almost every person on earth will benefit from learning some foundational ideas of statistics. This is true because statistics forms the basis of our everyday world just as much as do science, technology, and politics. Google, Netflix, Twitter, Facebook, OKCupid, Match.com, Amazon, iTunes, and the Federal Government are just a handful of the companies and organizations that use statistics on a daily basis. Journalism, political science, biology, sociology, psychology, graphic design, economics, sports science, and dance are all disciplines that have made use of statistical methodology. Course Material The materials on this website and in the lab manual will introduce you to the seminal ideas underlying the discipline of statistics. In addition, they have been designed with your learning in mind. For example, many of the class activities were developed using pedagogical principles, such as small group activities and discussion, that have been shown in research to improve student learning. Course readings should be completed outside of class and are intended to help you learn and extend the ideas, skills, and concepts you learn in the classroom. The readings themselves are not all “traditional” readings in the sense of words written on the screen, but instead often link to video clips, blogs and other multimedia material. The lab manual can be obtained here. You will need to bring a copy of the lab manual (physical or electronic) with you to class every day. TinkerPlots™ Software Much of the material presented in the lab manual requires the use of TinkerPlots™. This software can be downloaded (for Mac or PC), and a license can be purchased from http://www.tinkerplots.com/. Data Sets The data sets used in the Lab Manual, as well as those used in EPsy 3264 assignments, are available at https://github.com/zief0002/Statistical-Thinking/archive/master.zip. Clicking this link will download a ZIP file to your computer. This file includes all the data sets and the Lab Manual. Double-click on the ZIP file to view all the materials. Participation in the Learning Process The lab manual, instructors, and teaching assistants are all resources that are at your disposal to help you learn the material. In the end, however, you will have to do all of the hard work associated with actually learning that material. To successfully navigate this process, it is vital that you be an active participant in the learning process. Coming to class, participating in the activities and discussions, reading, completing the assignments, and asking questions are essential to successful learning. Learning anything new takes time and effort and this is especially true of learning statistics, as you are not just learning a set of methods, but rather a disciplined way of thinking about the world. Changing your habits of mind will take continual practice. It will also take a great deal of patience and persistence. As you engage in and use the skills, concepts and ideas introduced in the material, you will find yourself thinking about data and evidence in a different way. This may lead you to make different decisions or choices. But, even if this course does not change your world overnight, you will at the very least be able to critically think about inferences and conclusions drawn from data. Watch Joy of Stats online at http://www.gapminder.org/videos/the-joy-of-stats/↩ "],
["unit-1-modeling-simulation.html", "Unit 1: Modeling &amp; Simulation", " Unit 1: Modeling &amp; Simulation There is mounting evidence that the “model-building era”&quot; that dominated the theoretical activities of the sciences for a long time is about to be succeeded or at least lastingly supplemented by the “simulation era”.2 Modeling is one of the most important subjects you may ever learn. It is used in microbiology, macroeconomics, urban studies, sociology, psychology, public health, computer science, and of course, statistics. In fact, modeling is a method that is used in almost every discipline. Many think that it is an important skill to learn because it is so pervasive. While this is true, even more important is how closely the skills of modeling tie to the more general skills of problem solving. Starfield, Smith, and Bleloch (1994) summed this sentiment up nicely when they wrote, “learning to model is bound up with learning to solve problems and to think imaginatively and purposefully” (p. x).3 A model is a simplified representation of a system that can be used to promote an understanding of a more complex system. For example, meteorologists use computers to build models of the climate to understand and predict the weather. The computer model includes behaviors or properties which correspond, in some way, to the particular real-world system of climate. The computer models, however, do not include every possible detail about climate. All models leave things out and get some things—many things—wrong. This is because all models are simplifications of reality. Since all models are simplifications of reality there is always a trade-off as to what level of detail is included in the model. If too little detail is included in the model one runs the risk of missing relevant interactions and the resultant model does not promote understanding. If too much detail is included in the model, the model may become overly complicated and actually preclude the development of understanding. Models have many purposes, but are primarily used to better understand phenomena in the real-world. Common uses of models are for description, exploration, prediction, and classification. For example, Google builds models to understand and predict peoples’ internet searching tendencies. These models are then used to help Google carry out more efficient and better searches of information. As another example, Netflix builds models to understand the characteristics of movies that their customers have rated highly so that they can then recommend other movies that the person may enjoy. Amazon and Apple iTunes both use models in similar manners. Outline and Goals of Unit 1 The following schematic outlines the course readings, in-class activities, and assignments for Unit 1. In this unit, you will begin by exploring ideas of randomness. Randomness permeates, and is, in fact, fundamental to statistics. Then, you will learn how to use TinkerPlots™ to model several random processes and generate outcomes from those models. By generating data from different models, you will gain experience in considering the variation in outcomes that is produced by these random processes. This consideration will help you understand and overcome many misleading human intuitions about randomness. You will also be introduced to the Monte Carlo simulation process and learn how to carry out a Monte Carlo simulation using TinkerPlots™. This process allows you to quickly generate multiple data sets from a model in order to carry out hypothetical experiments. For example, we could ask the question: How likely is it to rain three out of the five days on my vacation given a particular forecast? By modeling the forecast and repeatedly generating data for the five days of vacation, we can then answer this question. As you progress through the unit, remember that the modeling process is a creative process that can often be very challenging. At times, this might lead to frustration as you are learning and practicing some of the material. But, as Mosteller et al. (1973) remind us, it is also a profitable experience since, “modeling is not only a technique of statistics…it is a method of study which can be applied in many other fields as well” (p. xii).4 Hartmann, S. (2005). The world as a process: Simulations in the natural and social sciences. http://philsci-archive.pitt.edu/2412/↩ Starfield, A. M., Smith, K. A., &amp; Bleloch, A. L. (1994). How to model it: Problem solving for the computer age. Edina, MN: Burgess International Group, Inc.↩ Mosteller, F., Kruskal, W. H., Link, R. F., Pieters, R. S., &amp; Rising, G. R. (1973). Statistics by example: Finding models. Reading, MA: Addison–Wesley.↩ "],
["randomness.html", "Randomness", " Randomness To begin your statistical journey, we will have you watch the following YouTube video: Random Sequences: Human vs. Coin "],
["generating-data-from-models.html", "Generating Data from Models", " Generating Data from Models One core skill of a practicing statistician is to be able to generate random data from a model. Most of the models you will encounter in this course are referred to as probability models. That is just a fancy way of associating probabilities with different events, or outcomes, in a model. For example, the model of flipping a “fair” coin is a probability model. There are two events/outcomes in the model: heads and tails. Each of these outcomes has a probability of 0.5 associaited with it. (Note that although we could say 50%, that probabilities are on the scale from 0 to 1, so are defined using decimal values.) In the in-class activity, Generating Random Data—Cat Factory, you will create several probability models to generate data about cats. To prepare for this activity, watch the following TinkerPlots™ tutorial video: Probability Simulation "],
["monte-carlo-simulation.html", "Monte Carlo Simulation", " Monte Carlo Simulation Monte Carlo simulation is one method that statisticians use to understand real-world phenomena. In Monte Carlo simulation, a model is used to generate multiple (sometimes millions) of data sets. By examining the data sets produced (or summaries of the data sets produced), researchers can draw insight about and predict what might happen in the real-world under a given set of circumstances. You can read about the fascinating origins of Monte Carlo simulation in the following article: The Beginning of the Monte Carlo Method Example of a Simulation Study In 1978, China introduced the “one-child” policy in order to alleviate social, economic, and environmental problems in China. According to Wikipedia,5 The policy officially restricts the number of children married urban couples can have to one, although it allows exemptions for several cases, including rural couples, ethnic minorities, and parents without any siblings themselves. A spokesperson of the Committee on the One-Child Policy has said that approximately 35.9% of China’s population is currently subject to the one-child restriction. Although the Chinese government has suggested that the policy has prevented more than 250 million births from its implementation to 2000, the policy is controversial both within and outside of China because of the manner in which the policy has been implemented. There have also been concerns raised about potential negative economic and social consequences, in part because many families were determined to have a son. Scholars have wondered how things would change if instead of a one-child policy, a country adopted a “one son” policy. A “one son” policy would allow families to keep having children until they had a son. If a family’s first child is a boy, they would be restricted from having more children. If, however, the first child was a daughter, the family could continue having children until a son was born. For example, they might ask the question, If China adopted a “one son” policy, how would the policy affect the average number of children per family, which is currently 1.66? One way in which this question could be studied (without actually implementing the policy) would be to conduct a simulation study by modeling this situation and generating many data sets from the model. Consider for a minute how you might model the number of children a particular family would have. One way to model this is to write the word boy on one index card and the word girl on another index card and to place those two index cards in a hat. After mixing up the index cards, you could draw a single card from the hat. If the card has the word boy written on it, the simulated “family” would be reported to have one child. If the card has the word girl written on it, a tally mark could be recorded and the index card would be replaced in the hat. The cards could then be remixed and another card would be drawn. If the card drawn has the word boy written on it, the simulated “family” would be reported to have two children. If the card has the word girl written on it, another tally mark could be recorded and the index card would again be replaced in the hat. This process would continue until the boy card was drawn. The table below shows the results after carrying out this process for three simulated families. Table 1: The recorded number of girls and boys for three simulated families. Family Girl Boy Family #1 ✔ ✔ Family #2 ✔ Family #3 ✔✔ ✔ We could carry out this simulation for many families, say 500 families, and use the results to provide an answer to the research question. You can imagine that carrying out even this simple simulation would quickly become quite tedious. Simulation studies, such as this, are typically carried out using computer programs. In this unit, you will learn to use a computer program called TinkerPlots™ to model processes in the real-world and carry out simulation studies. Simulation Assumptions “Wait,” you say. “Even if I carried out this simulation, I still would not be able to provide an answer to the research question! It doesn’t reflect reality! Some families may not want to have any children, while others might be happy to stop after a girl was born. What about multiple births?”&quot; Maybe you are even questioning whether the probability of having a boy or having a girl is really 50:50. These are all valid points, and all would likely affect the results of the simulation, which in turn affects the inferences and conclusions that are drawn. While the model used in the “one son” example is overly simplistic for drawing any sorts of meaningful conclusions about implementing such a policy in China, it could however, provide a useful starting point for introducing additional complexity. Even in the most enormously complicated modeling problem, researchers often make many simplifying assumptions. (Remember that all models—even those that seem quite complex—are simplifications of reality and get many things wrong.) With enough simplification, a model can be constructed and studied. The model is evaluated and often revised or updated as certain assumptions are deemed tenable and others are not. Because of this process, simulation studies are generally iterative in their development. This iteration process continues until an adequate level of understanding is developed and the research question can be answered. Simulation in Practice In practice, statisticians often use incredibly complex models to generate their data. As an example, Electronic Arts, the video game company behind titles such as Madden, NHL and FIFA, uses game telemetry (the transmission of data from a game executable for recording and analysis) to model the gameplay patterns of players and identify the elements of their games that are highly correlated with player retention.7 By understanding the behavior of players and the common patterns that are used, Electronic Arts game developers can focus their attention on more relevant features in future iterations of the game and ultimately reduce production costs. For example, in their examination of Madden NFL 11, Electronic Arts used 46 features to model players’ preferences, including their control usage, performance, and play-calling style. This is but one example of using simulation in video games. To see other applications of how data are being used in video game design, watch the webinar, How Big Data and Statistical Modeling are Changing Video Games. One-child policy. (2015, May 30). In Wikipedia, The Free Encyclopedia. Retrieved 18:02, June 1, 2015, from http://en.wikipedia.org/w/index.php?title=One-child_policy&amp;oldid=664745432↩ World Factbook↩ Weber, B. G., John, M., Mateas, M., &amp; Jhala, A. (2011). Modeling player retention in Madden NFL 11. Presented at Innovative Applications of Artificial Intelligence. http://users.soe.ucsc.edu/~bweber/pubs/madden11retention.pdf↩ "],
["unit-2-hypothesis-evaluation.html", "Unit 2: Hypothesis Evaluation", " Unit 2: Hypothesis Evaluation In the course activities and homework assignments, you have been using probability models to generate random outcomes. You have also learned how to use Monte Carlo simulation to generate many data sets from a given model. This is the same kind of process that researchers, scientists, and statisticians engage in when they evaluate (or test) hypotheses about the world. To illustrate the ideas behind statistical hypothesis testing, consider how you might go about testing a coin for “fairness”. You might have suggested something along the lines of “flip the coin many times and keep track of the number of heads and tails”. Suppose you tossed the coin 100 times, which resulted in 53 heads and 47 tails. Would you say the coin is “unfair”? What if you had obtained 65 heads and 35 tails instead? Now would you say the coin is “unfair”? How about if you had gotten 84 heads and only 16 tails? The first result of 53 heads and 47 tails probably did not seem that far fetched to you, and you probably would feel at ease saying that the coin that produced such a result is most likely “fair”. On the other hand, the results of 65 heads and 35 tails—and especially 84 heads and 16 tails—likely made you feel uncomfortable about declaring the coin “fair”. Why is this? It is because you had a mental model of the distribution of heads and tails that you expect when the coin actually IS “fair”. For most people, this mental model encompasses a uniform distribution of the outcomes (e.g., a 50:50 split between heads and tails). If the observed result of the 100 coin flips is consistent with the model of a “fair” coin, you might conclude that the coin is “fair”. For example, the result of 53 heads from 100 flips is very close to the 50:50 split of heads and tails, and it is probably safe to say that a “fair” coin could have produced the set of flips in question. In this case, the data are consistent with the model of “fairness”. If the observed result deviates from what is expected under the model of a “fair” coin, for example the two results of 65 heads and 84 heads, you might end up rejecting the hypothesis that the coin was “fair”. In these two cases, the data are inconsistent with the model of “fairness”. One thing you may have realized is that we expect variation in the results just because of chance (randomness). For example, if you flipped 100 coins over and over (say, 500 times), you would not get 50 heads every time. Knowing the amount of variation that is expected is how we can judge whether data are consistent or inconsistent with the model. Luckily, we can model the expected variation using Monte Carlo simulation. Simulation Process for Evaluating Hypotheses The process we will use for evaluating a hypothesis is: Create a model that conforms to the hypothesis to be evaluated. Use the selected model to generate many, many sets of data (Monte Carlo simulation). The results you collect and pool together from these trials will give a picture of the variation you would expect under the hypothesized model. Evaluate whether the results observed in the actual data (not the simulated data) are consistent with the expected results produced from the model. This acts as evidence to either support or dispute the hypotheis. To help you remember this process, you can use the more simplistic: Model Simulate Evaluate This may sound like a straight-forward process, but in practice it can actually be quite complex—especially as you are reading research articles and trying to interpret the findings. First off, the model that is selected is often not provided, nor described, explicitly within most research articles. It is often left to the reader to figure out what the assumed model was. At first, this may be quite difficult, but like most tasks, as you gain experience in this course and as you read more research, you find that there are a common set of models that are typically used by researchers. The model that you use in the Monte Carlo simulation is directly related to the hypothesis you make about a research question. Often researchers explicitly state hypotheses about their research questions. Hypotheses are simply statements of possible explanations for an observed set of data. For example, one possible explanation for the observed set of coin flips is: The coin used to generate the set of observed coin flips is a “fair” coin; which would produce (in the long run) a uniform distribution of heads and tails. One complication that you may encounter is that many statisticians and researchers write their hypotheses mathematically. The advantage to writing a hypothesis mathematically is that it explicitly defines the model that will be used in the Monte Carlo simulation. Consider the stated hypothesis that the coin used to generate the set of observed coin flips is a “fair” coin that produces a uniform distribution of heads and tails. Recall that producing a uniform distribution of heads and tails means that heads and tails are equally likely under this model (i.e., a 50:50 split). We could express this hypothesis more mathematically as: The model produces heads (and tails) with a probability of 0.5. Symbolically, we would express this hypothesis as: \\[ H_0: \\pi_{\\mathrm{Heads,~Tails}} = 0.5 \\] The symbol \\(H_0\\) is common and indicates a hypothesis about a model. Here, \\(\\pi\\) is the greek letter pi and means “probability” or “proportion”. (Typically in symbolic notation for hypotheses, pi is not the mathematical constant of 3.14.) Statisticians typically use Greek letters to identify probabilities of the outcomes in a model.8 In this hypothesis, we are establishing that the model we are evaluating generates heads (and tails) with a probability of 0.5. Notice how the model is completely defined using the mathematical notation. The hypothesis states that the model has two potential outcomes (heads and tails), and the probability of each is 0.5. Pretty cool, huh? Although the more qualitative hypothesis that the coin used to generate the set of observed coin flips is a “fair” coin that produces a uniform distribution of heads and tails may seem more understandable (at least right now), If this all seems like gibberish to you right now, do not worry about it. You can always write hypotheses descriptively, without resorting to the symbolic notation. Remember, we wrote the EXACT SAME hypothesis three different ways. If you are comfortable with the mathematical symbols, use it; the mathematical notation acts as a shorthand to quickly state a hypothesis and define the model used. As you read research articles or take other courses, you will see statistical hypotheses stated in many ways, so it is good to understand that there are many ways to express the same thing. Outline and Goals of Unit 2 The following schematic outlines the course readings, in-class activities, and assignments for Unit 2. In the readings, course activities, and assignments in Unit 2, you will explore the process of evaluating statistical hypotheses. You will be introduced to several common models that used by researchers and statisticians. You will also use TinkerPlots™ to generate simulated data to study the variation in results that would be expected under these models. Many of these models are directly related to the chance models that you have explored in the course to this point. For example, you should already be able to use TinkerPlots™ to produce results that would be expected from 100 flips of a “fair” coin. Aside from learning about some of the more common models used in research, you will also learn how to describe and formally quantify the variation in a distribution. This is helpful as we evaluate whether a particular result in observed data is consistent with results produced from the given model. Lastly, you will learn about common misconceptions regarding model evaluation (e.g., we can never say a model produced the data, only that it produces results consistent with the data), and how to use probabilistic languge when providing an “answer” to a research question. Greek vs. Roman Letters Greek letters are used when the parameters of a model are being described. In contrast, Roman letters are used to describe observed results. For example, go back to the situation in which the observed data consisted of 53 heads and 47 tails from 100 flips of a coin. Here we would say \\(p_{\\mathrm{Heads}} = 0.53\\). The hypothesis about the model we are evaluating produces heads with a probability of 0.5, so \\(\\pi_{\\mathrm{Heads,~Tails}} = 0.5\\). Rather than use the Roman letters, some statisticians prefer to put a “hat” on the Greek letter to refer to the observed result. For example, \\(\\hat{\\pi}_{\\mathrm{Heads}}=0.53\\). In this course we are not as concerned about which notation you use to express the result observed in the actual data. In fact, it might be less confusing if you just write, the observed result is 0.53.↩ "],
["describing-distributions.html", "Describing Distributions", " Describing Distributions One of the important steps in any statistical analysis is that of summarizing data. It is good practice to examine both a graphical and a numerical summarization of your data. These summarizations are often part of the evidence that researchers use to support any conclusions drawn from the data. They also allow researchers to discover structure that might have otherwise been overlooked in the raw data that was actually collected. Lastly, both graphical and numerical summaries of the data often point to other analyses that may be undertaken with the data. Once raw data has been collected in a study, it can be overwhelming to pull any kind of meaning out of it. For example, it is not uncommon for Google to be dealing with millions of cases. How can Google—or any researcher for that matter—go from all of that raw data to something that can help them answer their research questions? Rather than examining all of those cases individually, researchers examine the data collectively, often by plotting it. This is what is meant by a graphical summary of the data; it is quite literally, a picture of the distribution. There are many, many different types of plots that have been created to graphically summarize data. Each can provide a slightly different representation of the data. Metaphorically, you can imagine each of these different plot types as a different photo taken of the exact same person. Some may be color, others black and white. Some may be taken from different perspectives, angles or distances. While all photographs “summarize” the same person, you may notice characteristics of that person in some photos that are not evident in others. Many of the photos, however, will show the same thing. Shape The dot plot that TinkerPlots™ provides is a very useful plot.9 It allows us to summarize the shape of the distribution very easily. Shape is used to describe a distribution’s symmetry. As you might expect, symmetric distributions are shaped the same on either side of the center. (Another way of thinking about this is that if you folded the distribution at the center, the folded half of the distribution would align pretty well on top of the other half.) For example, “bell-shaped” (“approximately normal”) distributions are symmetric. When a distribution is asymmetric, it is referred to as a skewed distribution. The distribution shown in Figure 1 is a skewed distribution. In this distribution, there appears to be a longer tail on the right side of the distribution. Because the tail is on the right side of the distribution, statisticians would say it is “skewed to the right” or “positively skewed”. In a similar way, a distribution that tails to the left is “skewed to the left” or “negatively skewed”. Figure 1: This distribution is skewed to the right, or positively skewed. Location Aside from the overall shape of the distribution, it is also useful to summarize the location of the distribution. The location of the distribution provides a summarization of a so-called “typical” value for the data. A “typical” value can be estimated from the plot of the distribution. You can also use more formally calculated summaries of the location such as the mean, median, or mode. These values are easily calculated using TinkerPlots™. When looking at a plot of a distribution, data analysts often consider the number of modes or “humps” that are seen in a plot of the distribution. Here, the concept of mode is slightly different (although related) to the concept of mode that you may have learned in previous mathematics or statistics courses. The mode of a distribution gives a general sense of the values or measurements that occur frequently. This may be a single number, but many times is not. For example, the first hump of the distribution shown in the figure below suggests that values around nine are very common. The actual value of nine, however, may only show up once or twice in the data. Figure 2: A bimodal distribution showing two modes. One mode is around 9, and the other is near 12. A distribution can be unimodal (one mode), bimodal (two modes), multimodal (many modes), or uniform (no modes). The distribution shown above is bimodal—notice there are two humps. Uniform distributions have roughly the same frequency for all possible values (they look essentially flat) and thus have no modes. Variation The other characteristic of a distribution that should be summarized is the variation. Summarizing the variation gives an indication of how variable the data are. One method of numerically summarizing the variability in the data is to quantify how close the observations are relative to the “typical” value on average. Are the observations for the most part close to the “typical” value? Far away from the “typical” value? How close? It turns out, that the shape of the distribution also helps describe the variation in the data. For example, “bell-shaped” distributions have most observations close to the “typical” value, and more extreme observations show up both below and above the “typical” value (the variation is the same on both sides of the “typical” value). Whereas skewed distributions have many observations near the “typical” value, but extreme values only deviate from this value in one direction (there is more variation in the data on one side of the “typical” value than the other). Figure 3: Most of the observations in this distribution are clustered between 0 and 2. There are some observations greater than 2 (up to 10), although these are rare. One thing that affects the variation, and should be described is whether there are observations that stand out from the other observations. Often these observations have extremely large or small values relative to the other observations. These observations are referred to as potential outliers, or extreme cases. For example, in the positively skewed distribution shown previously, the observation that has a value near 10 would likely be considered a potential outlier. Putting It All Together Rotten Tomatoes is a website which aggregates movie critics’ reviews of films. The website marks each review as either positive or negative and then gives the film a score based on the percentage of positive reviews. In addition to the critics’ score, each film is also given a score based on reviews from the general public using the same methodology (reviews are tabulated so that the score represents the percentage of positive reviews from the general public). The plot shown below shows a dot plot of the distribution of the general public’s scores for 134 movies released in 2009. Figure 4: The scores for 134 movies released in 2009 based on the general public’s reviews. The scores represent the percentage of positive reviews for each movie. A written description of the distribution might read as follows: The distribution of scores for this sample of 134 movies is fairly symmetric. The median score for these movies is near 60, indicating that a typical movie released in 2009 is positively reviewed by about 60% of the public. The distribution also indicates that there is a lot of variation in the movies’ scores. Most of the movies in the sample have a score between 35 and 85, suggesting large differences in the public’s opinion of the quality of these movies. Notice that the description incorporates the context of the data, in this case film scores. This helps a reader to interpret the description. TinkerPlots™ also provides other types of plots, including the box plot (sometimes called the box-and-whiskers plot) and the hat plot (a variation of the box plot).↩ "],
["unit-3-randomization-tests.html", "Unit 3: Randomization Tests", " Unit 3: Randomization Tests The nature of doing science, be it natural or social, inevitably calls for comparison. Statistical methods are at the heart of such comparison, for they not only help us gain understanding of the world around us but often define how our research is to be carried out.10 Drawing inferences about the differences between groups is an almost daily occurrence in the lives of most people. In any given hour of any given day, television, radio and social media abound with comparisons. For example, data scientists at OKCupid, an online dating site, examined whether frequent tweeters (users of Twitter) have shorter real-life relationships than others.11 Group comparisons are at the heart of many interesting questions addressed by psychologists, physicians, scientists, teachers, and engineers. Questions about group differences are often studied through scientific experiements. When considering a scientific experiment to examine group differences, the design of the study plays a very important role. To help understand this, think about a researcher who is studying the efficacy of a new cold medication. Let’s say that the researcher has 100 people (each with a cold) who volunteer to be a part of her study. Let’s consider how she might design her study. Design 1: She gives the cold medicine to all 100 volunteers. Design 2: She gives the cold medicine to the first 50 volunteers (treatment group) and nothing to the other 50 volunteers (control group). Design 3: She randomly picks 50 volunteers to whom she gives the cold medicine (treatment group), and she gives nothing to the other 50 volunteers (control group). All three designs have been used, and are still used, in research studies. There are pros and cons to each of the designs, and all are useful depending on what you want to know. In Design 1, it is hard to judge the efficacy of the medication. For example, what if 60 of the volunteers had no cold symptoms after four days. Did the medication work? You might be thinking, “what would have happened if they hadn’t received any medication?” That is a great question. In this design, we don’t know. Design 2 gives the researcher a comparison group. She can compare the number of volunteers in each group who have no cold symptoms after four days. This is a better design than Design 1 for examining efficacy. But, what if she found that after four days, 35 of the volunteers who got the medication had no symptoms, while only 25 of the volunteers who didn’t receive medication had no symptoms. Is this enough evidence for her to say the cold medication is effective? Probably not. Maybe most of the volunteers in the treatment group were already in later stages of their colds. Maybe they had more robust immune systems to begin with (e.g., due to differing exercise or nutrition habits) than the control group. You can imagine many such reasons that the treatment group would show quicker improvement than the control group. Design 3 has the same comparison group advantage as Design 2. The big difference, however, is that the volunteers were put into the groups at random. By assigning participants at random, the researcher “equalizes” the treatment and control groups. (You will study this more in Unit 4.) For now, what this means is that the groups have, on average, the SAME nutritional habits, the SAME exercise habits, and the SAME everything-else. That means that the only thing that is different between the two groups is that the treatment group got the cold medication and the control grou didn’t. If the researcher uses this type of design, she can draw much stronger inferences about WHY the treatment group improved: it was because of the cold medication! Experimental Variation Let’s say our hypothetical researcher used a srong design in which she randomly assigned her volunteers to treatment and control groups. After four days she found that the treatment group had 35 of the 50 volunteers with no symptoms, and the control group had 27 of 50 volunteers with no symptoms. Could she conclude that the cold medication is effective since 8 more volunteers had no symptoms in the treatment group? Actually no. And, the reason is because of experimental variation. Consider the situation where the treatment has absolutely NO EFFECT. In other words, it does nothing. Under that assumption, the treatment and the control groups should improve at about the same time. Differences between the treatment and control group are not a function of the cold medication. They are solely a function of random chance. Similar to the studies we looked at in Unit 2, we have to figure out how much chance variation is expected before we can say whether the difference of 8 volunteers is actually an improvement. One key difference between this type of study and those in Unit 2 is that the chance variation arises from the assignment to groups in these studies, whereas in Unit 2, the chance variation arose because of sampling from a larger population. When the chance variation is due to the assignment of participants to groups, it is referred to as experiemental variation rather than sampling variation. Outline and Goals of Unit 3 The following schematic outlines the course readings, in-class activities, and assignments for Unit 3. In the readings, course activities, and assignments in Unit 3, you will explore the process of modeling experimental variation to be able to evaluate observed differences between groups. You will learn about the randomization test, a Monte Carlo method for modeling experimental variation, and how to carry out this test using TinkerPlots™. Liao, T. F. (2002). Statistical group comparison. New York: Wiley.↩ The website OKTrends includes an answer to this question, as well as many others.↩ "],
["quantifying-results-p-value.html", "Quantifying Results: p-Value", " Quantifying Results: p-Value In addition to computing the range of likely results from the model, statisticians also typically provide a quantification of the likelihood of the observed result given the hypothesized model. This quantification is referred to as a \\(p\\)-value (the \\(p\\) stands for probability). To compute a \\(p\\)-value, you count the number of results that are at least as extreme as the observed result, and divide this by the total number of results. \\[ p = \\frac{\\mathrm{number~of~results~more~extreme~than~observed}}{\\mathrm{total~number~of~simulated~results}} \\] This value is then reported as a decimal value. It quantifies the probability of observing a result at least as extreme as the observed result under the hypothesized model. To illustrate this, we will re-examine simulation results from the Sleep Deprivation study. Recall in that activity, the observed data had a difference in means of 15.9. Below is a plot of 100 differences in means simulated under the “no-effect”&quot; model. A vertical line is shown at the observed difference of 15.9. Since 15.9 is to the right of 0 (i.e., it is on the right-hand side of the plot) results that are more extreme than the observed result are to the right of 15.9. (If the observed result was to the left of 0, more extreme results would be those more negative than the observed result.) Here there are two simulated results out of 100 that are at least as extreme as 15.9 (\\(\\geq 15.9\\)). We would report the \\(p\\)-value as 0.02. Adjustment for Simulation Results In simulation studies, we make one small adjustment to the \\(p\\)-value computation; we add 1 to both the numerator and denominator: \\[ p = \\frac{\\mathrm{number~of~results~more~extreme~than~observed} + 1}{\\mathrm{total~number~of~simulated~results} + 1} \\] This adjustment assures that we never get a \\(p\\)-value of 0. Consider the \\(p\\)-value if our observed result would have been 18 (instead of 15.9). There are 0 results that are at least as extreme as 18 (\\(\\geq 18\\)). Without making the simulation adjustment, we would report a \\(p\\)-value of 0. This implies that seeing a result at least as extreme as 18 under the “no-effect” model is impossible. The problem is that we only ran 100 trials of the simulation. If we had run this simulation for all possible randomizations of the data, we would have seen results \\(\\geq 18\\). So, to report a \\(p\\)-value of 0 is misleading. The \\(p\\)-value we report should be, \\[ p = \\frac{0 + 1}{100 + 1} = 0.0099 \\] After the adjustment, the \\(p\\)-value is still quite small, indicating that had we seen an observed result of 18, we would say that it is inconsistent with the model of “no-effect”. In fact it is in the outer 0.01 (1%) of the results simulated from the hypothesized model. Going back to the \\(p\\)-value computed from the observed value of 15.9, \\[ p = \\frac{2 + 1}{100 + 1} = 0.0297 \\] We can interpret the \\(p\\)-value of 0.030 as indicating that the observed difference of 15.9 is in the outer 0.03 (3%) of results simulated from the hypothesized model. It is quite unlikely that we would see a result as extreme as 15.9, or more extreme, under the hypothesized model of “no effect”. Level of Evidence Small \\(p\\)-values are used as evidence against the hypothesized model. While there are no hard-and-fast rules for gauging how strong the evidence is against the hypothesized model, the following guidelines can be used: A \\(p\\)-value above 0.10 constitutes little to no evidence against the hypothesized model. A \\(p\\)-value between 0.05 and 0.10 constitutes borderline/weak evidence against the hypothesized model. A \\(p\\)-value between 0.025 and 0.05 constitutes moderate evidence against the hypothesized model. A \\(p\\)-value between 0.001 and 0.025 constitutes substantial/strong evidence against the hypothesized model. A \\(p\\)-value below 0.001 constitutes overwhelming evidence against the hypothesized model. In the Sleep Deprivation study, the \\(p\\)-value of 0.03 would constitute moderate evidence against the hypothesized model of “no-effect”. Six Principles about p-Values Because they are so ubiquitous in the research literature for any field, and because they are often mis-interpreted (even by PhDs, researchers, and math teachers) it is important to be aware of what a \\(p\\)-value tells you, and more importantly, what it doesn’t tell you. To this end, the American Statistical Association released a statement on \\(p\\)-values in which it listed six principles:12 Principle 1: \\(P\\)-values can indicate how incompatible the data are with a specified statistical model. Principle 2: \\(P\\)-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone. Principle 3: Scientific conclusions and business or policy decisions should not be based only on whether a \\(p\\)-value passes a specific threshold. Principle 4: Proper inference requires full reporting and transparency. Principle 5: A \\(p\\)-value, or statistical significance, does not measure the size of an effect or the importance of a result. Principle 6: By itself, a \\(p\\)-value does not provide a good measure of evidence regarding a model or hypothesis. Principle 3 is especially poignant, as many reserachers use the criterion of 0.05 when evaluating \\(p\\)-values to indicate whether a result is statistically significant (\\(p \\leq 0.05\\)) or not. In fact, this use was so pervasive and problematic, that the journal Basic and Applied Social Psychology banned the use of \\(p\\)-values in its published articles. It also led to many jokes about \\(p\\)-values, including this XKCD comic. Yaddanapudi (2016) published a paper in the Journal of Anaesthesiology, Clinical Pharmacology in which she explains each of these six principles for practicing physician-scientists using an example of treatment efficacy for a drug.↩ "],
["unit-4-design-and-inference.html", "Unit 4: Design and Inference", " Unit 4: Design and Inference How studies are designed and carried out play a large role in the inferences that can be drawn. For example, randomly assigning participants to experiemental groups plays a large role in the degree to which cause-and-effect inferences can be drawn. According to Wikipedia, Random assignment of participants helps to ensure that any differences between and within the groups are not systematic at the outset of the experiment. Thus, any differences between groups recorded at the end of the experiment can be more confidently attributed to the experimental procedures or treatment. It is for this reason that random assignment is often referred to as the “Gold Standard” of design. While random assignment is the strongest of the experimental designs for attributing cause, it is not perfect. The same Wikipedia page goes on to say, Random assignment does not guarantee that the groups are matched or equivalent. The groups may still differ on some preexisting attribute due to chance. The use of random assignment cannot eliminate this possibility, but it greatly reduces it. Outline and Goals of Unit 4 The following schematic outlines the course readings, in-class activities, and assignments for Unit 4. In the readings, course activities, and assignments in Unit 4, you will explore how random assignment “equalizes” groups, and why it sometimes is not a perfect equalizer. You will also gain more experience with consideration of study design and the role it plays in inference. Finally, you will learn what to do when a study doesn’t employ random assignment. --> --> --> --> --> --> Is the complexity of speeches given by Democrats different than that of speeches given by Republicans? --> --> --> --> "]
]
